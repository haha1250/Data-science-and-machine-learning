{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Q3_CNN_FireClassification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsSHtYhj57oH"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1zswaNO57oO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEEm7vrE57oQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ak19ois57oR"
      },
      "source": [
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3,3), input_shape = (64, 64, 3), activation=\"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hY2eJqW57oS"
      },
      "source": [
        "# step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlvRvVn57oS"
      },
      "source": [
        "# adding a second convolution layer\n",
        "classifier.add(Conv2D(32, (3,3), activation=\"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzG3Fspq57oT"
      },
      "source": [
        "# step 3 - Flattening\n",
        "classifier.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiErPZS57oU"
      },
      "source": [
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezV_57OQ57oV"
      },
      "source": [
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO21s-jZ57oW",
        "outputId": "9f0ab85c-f2b8-4cb7-9a19-563006ac5c28"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 813,217\n",
            "Trainable params: 813,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrGn_3it57oY"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wKB8em557oa"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip=True\n",
        "                                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYO_eWlq57oa"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsLQKKLv57oa",
        "outputId": "173906d3-6cc0-45b2-8d7b-ea085c961620"
      },
      "source": [
        "training_set = train_datagen.flow_from_directory('images/training_set',\n",
        "                                                target_size = (64,64),\n",
        "                                                batch_size = 32,\n",
        "                                                class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3264 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zefba08E57ob",
        "outputId": "f11aade8-ce22-4341-9d07-7ad2bad96c37"
      },
      "source": [
        "print(len(training_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrvaCq4757oc",
        "outputId": "1630ef58-6b87-4ee0-c476-0340cb544548"
      },
      "source": [
        "test_set = train_datagen.flow_from_directory('images/testing_set',\n",
        "                                                target_size = (64,64),\n",
        "                                                batch_size = 32,\n",
        "                                                class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 858 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ9sl4uh57od",
        "outputId": "3ec280c5-04f7-4dcc-d185-ba9f6819ba7d"
      },
      "source": [
        "print(len(test_set)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOl-CXxT57od"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I97Ayn257of"
      },
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
        "            ModelCheckpoint('fire_checkpoint_new.h5', save_best_only=True)\n",
        "            ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmjw9TkTkKzy"
      },
      "source": [
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4wObBdkL6h",
        "outputId": "7403da35-6fed-494c-d9e4-9154b31c49a2"
      },
      "source": [
        "t0 = datetime.datetime.now()\n",
        "print(t0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-11-29 13:33:58.927118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6NAo7q57og",
        "outputId": "2b51774f-4888-40c6-8141-820fef495cf2"
      },
      "source": [
        "# If batch_size = 1 => 1 epoch se co so batch = so mau\n",
        "classifier.fit(training_set, \n",
        "              steps_per_epoch = len(training_set),\n",
        "               epochs = 100,\n",
        "               validation_data = test_set,\n",
        "               validation_steps = len(test_set),\n",
        "               callbacks = callbacks\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "102/102 [==============================] - 126s 1s/step - loss: 0.4969 - accuracy: 0.7865 - val_loss: 0.4783 - val_accuracy: 0.7704\n",
            "Epoch 2/100\n",
            "102/102 [==============================] - 71s 692ms/step - loss: 0.3899 - accuracy: 0.8278 - val_loss: 0.4301 - val_accuracy: 0.8124\n",
            "Epoch 3/100\n",
            "102/102 [==============================] - 77s 750ms/step - loss: 0.3725 - accuracy: 0.8444 - val_loss: 0.4284 - val_accuracy: 0.8100\n",
            "Epoch 4/100\n",
            "102/102 [==============================] - 78s 767ms/step - loss: 0.3548 - accuracy: 0.8487 - val_loss: 0.4277 - val_accuracy: 0.7995\n",
            "Epoch 5/100\n",
            "102/102 [==============================] - 80s 787ms/step - loss: 0.3416 - accuracy: 0.8609 - val_loss: 0.4230 - val_accuracy: 0.8077\n",
            "Epoch 6/100\n",
            "102/102 [==============================] - 91s 894ms/step - loss: 0.3100 - accuracy: 0.8775 - val_loss: 0.4300 - val_accuracy: 0.8124\n",
            "Epoch 7/100\n",
            "102/102 [==============================] - 81s 789ms/step - loss: 0.2989 - accuracy: 0.8765 - val_loss: 0.4230 - val_accuracy: 0.8357\n",
            "Epoch 8/100\n",
            "102/102 [==============================] - 73s 719ms/step - loss: 0.2972 - accuracy: 0.8793 - val_loss: 0.3846 - val_accuracy: 0.8333\n",
            "Epoch 9/100\n",
            "102/102 [==============================] - 80s 787ms/step - loss: 0.2789 - accuracy: 0.8808 - val_loss: 0.4835 - val_accuracy: 0.7937\n",
            "Epoch 10/100\n",
            "102/102 [==============================] - 100s 984ms/step - loss: 0.2698 - accuracy: 0.8940 - val_loss: 0.4269 - val_accuracy: 0.8252\n",
            "Epoch 11/100\n",
            "102/102 [==============================] - 79s 778ms/step - loss: 0.2690 - accuracy: 0.8931 - val_loss: 0.4266 - val_accuracy: 0.8345\n",
            "Epoch 12/100\n",
            "102/102 [==============================] - 70s 682ms/step - loss: 0.2440 - accuracy: 0.9066 - val_loss: 0.3763 - val_accuracy: 0.8497\n",
            "Epoch 13/100\n",
            "102/102 [==============================] - 71s 698ms/step - loss: 0.2525 - accuracy: 0.8940 - val_loss: 0.4702 - val_accuracy: 0.8205\n",
            "Epoch 14/100\n",
            "102/102 [==============================] - 80s 789ms/step - loss: 0.2486 - accuracy: 0.9010 - val_loss: 0.3776 - val_accuracy: 0.8531\n",
            "Epoch 15/100\n",
            "102/102 [==============================] - 90s 884ms/step - loss: 0.2239 - accuracy: 0.9075 - val_loss: 0.4369 - val_accuracy: 0.8240\n",
            "Epoch 16/100\n",
            "102/102 [==============================] - 80s 780ms/step - loss: 0.2201 - accuracy: 0.9118 - val_loss: 0.3711 - val_accuracy: 0.8578\n",
            "Epoch 17/100\n",
            "102/102 [==============================] - 72s 701ms/step - loss: 0.2136 - accuracy: 0.9139 - val_loss: 0.3713 - val_accuracy: 0.8473\n",
            "Epoch 18/100\n",
            "102/102 [==============================] - 86s 841ms/step - loss: 0.2014 - accuracy: 0.9157 - val_loss: 0.3957 - val_accuracy: 0.8415\n",
            "Epoch 19/100\n",
            "102/102 [==============================] - 91s 888ms/step - loss: 0.1942 - accuracy: 0.9234 - val_loss: 0.4188 - val_accuracy: 0.8450\n",
            "Epoch 20/100\n",
            "102/102 [==============================] - 86s 846ms/step - loss: 0.1784 - accuracy: 0.9286 - val_loss: 0.4083 - val_accuracy: 0.8578\n",
            "Epoch 21/100\n",
            "102/102 [==============================] - 73s 716ms/step - loss: 0.1717 - accuracy: 0.9366 - val_loss: 0.4169 - val_accuracy: 0.8648\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x212b4b64748>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NswWzWJjlyXB",
        "outputId": "ff44cdd7-bcbf-436d-a942-8716748358ee"
      },
      "source": [
        "t1 = datetime.datetime.now()\n",
        "print(t1-t0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:29:12.264985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MYDUD_l57oh",
        "outputId": "412a32b3-7a50-4fb6-afc8-0b9a45ce1a8d"
      },
      "source": [
        "# save model\n",
        "from tensorflow.keras.models import load_model\n",
        "# Creates a HDF5 file 'my_model_CNN_new.h5'\n",
        "classifier.save('my_fire_CNN_new.h5')\n",
        "print(\"save!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92K1_-vH57oh"
      },
      "source": [
        "# them du lieu test ngoai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUuZ2YjE57oi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}